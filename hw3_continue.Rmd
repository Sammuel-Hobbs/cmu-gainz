---
title: "AA-MLP Homework 3: Neural networks for billing estimation"
author: "Sammuel Hobbs | shobbs"
date: "November 27, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# for some reason, its pointting to a new location 3.3????
.libPaths("C:/Users/Sammuel Hobbs/Documents/R/win-library/3.5")
```

### How might a prediction for DRG cost weight be useful for billing coders?
**Hypothetically, an accurate tool that estimates the payment multiplier could (a) detect instances that might be innaccurately billed or even (b) streamline the hospital's current processes used to bill insurance companies, which might lead to (i) reduced labor costs and (ii) reduced time duration for collecting payment (improved cash flow).**

### Preprocessing & Data Overview
##### Set up workspace, load libraries
```{r message=FALSE, warning=FALSE}
#echo=FALSE
setwd("C:/Users/Sammuel Hobbs/Desktop/Semester 3/Applied Analytics the Machine Learning Pipeline/HWK/HWK3/")
directory = "C:/Users/Sammuel Hobbs/Desktop/Semester 3/Applied Analytics the Machine Learning Pipeline/HWK/HWK3/data"

### Load helper files ###
loadlibs = function(libs) {
  for(lib in libs) {
    class(lib)
    if(!do.call(require,as.list(lib))) {install.packages(lib)}
    do.call(require,as.list(lib))
  }
}
libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "mice", "lubridate", "imager", "naniar")
#libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "lubridate")
loadlibs(libs)
```

#### Cost Weights, Labs, and Procedure Events that occurred among all hospital admits
##### Loading procedure files
```{r message=FALSE, warning=FALSE}
# fread strip.white=TRUE is default, so no worries on leading/trailing whitespace in d_lab
d_coditem = fread(paste0(directory, "/d_codeditems.csv")) %>% as_tibble()
str(d_coditem)
drgevents = fread(paste0(directory, "/drgevents.csv")) %>% as_tibble()
str(drgevents)
procevent = fread(paste0(directory, "/procedureevents.csv")) %>% as_tibble()
str(procevent)
```

#### Cost Weights Summary

```{r}
# select relevant features
d_sumry = drgevents %>%
  select(hadm_id, itemid, cost_weight)

# count how many times each cost weight has ocurred, creating cost_count column
d_sumry = d_sumry %>% 
  group_by(cost_weight) %>%
  mutate(cost_count = n())

cat("Total cost weights: ", dim(d_sumry)[1], "\n")

# Get distinct cost weights, and sort by cost count
d_sumry = d_sumry %>% 
  distinct(cost_weight, .keep_all = TRUE) %>%
arrange(desc(cost_count))

cat("Unique cost wieghts: ", dim(d_sumry)[1], "\n")

# Basic data summary - cost_weight distributions
#Hmisc::describe(d_sumry$cost_weight)
summary(d_sumry$cost_weight)

# Top ten
d_sumry %>% 
  select(cost_weight, cost_count) %>%
  head(10)

# bottom ten
d_sumry %>% 
  select(cost_weight, cost_count) %>%
  tail(10)
```

#### Procedure Events Summary

```{r}
# left join procedures and d_codeditems, keeping hadmin_id, description, itemid
procevent = procevent %>% 
  left_join(d_coditem, by="itemid") %>% 
  select(hadm_id, description, itemid)

# inner join drgevents and procedures, keeping cost_weight (y), hadmin_id, and description (to be Xs)
### drg_proc is full data set to be included in model
drg_proc = drgevents %>% 
  inner_join(procevent, by="hadm_id") %>% 
  select(cost_weight, hadm_id, description)
cat("Total procedures: ", dim(drg_proc)[1], "\n")

# count how many times each procedure has ocurred, creating proc_count column
drg_proc = drg_proc %>% 
  group_by(description) %>%
  mutate(proc_count = n())

# Get distinct descriptions, and sort by procedure count
proc_top = drg_proc %>% 
  distinct(description, .keep_all = TRUE) %>%
arrange(desc(proc_count))
cat("Total unique procedure descriptions: ", dim(proc_top)[1], "\n")

# Basic data summary - event count distribution
summary(proc_top$proc_count)

# Top ten
proc_top %>% 
  select(description, proc_count) %>%
  head(10)

# bottom ten
proc_top %>% 
  select(description, proc_count) %>%
  tail(10)
```

#### Lab Events Summary
```{r}
d_labitem = fread(paste0(directory, "/d_labitems.csv")) %>% as_tibble()
str(d_labitem)
labevents = fread(paste0(directory, "/labevents.csv")) %>% as_tibble()
str(labevents)
```

##### Join Lab Data sets, pre-process missing, feature engineering
```{r}
# filter to only include hadm_id present in DRG Events... 
# itemid.x is itemid from labevents
labev_drg = labevents %>% 
  inner_join(drgevents, by="hadm_id") %>%
      select(hadm_id, itemid.x, flag)
cat("Total lab events: ", dim(labev_drg)[1], "\n")

### Expert knowledge Assumption: only abnormal events are recorded as abnormal, so assume NA is normal
# reset column name, then encode flag NA to normal
labev_drg = rename(labev_drg, itemid = itemid.x)
labev_drg = labev_drg %>%
  mutate(flag  = ifelse(is.na(flag), "normal", flag))

#Join for descriptions, d_labitem to get descriptions
### labs_labels to be included in Model
labs_labels = labev_drg %>% 
  inner_join(d_labitem, by="itemid") %>%
  select(hadm_id, itemid, test_name, fluid, category, loinc_description, flag)

### Where loinc_description is missing, create a compound description which is combination of test_name, fluid, category
### Description encoded as "miss_desc_test_name_fluid_category"
labs_labels = labs_labels %>%
  mutate(loinc_description  = ifelse(is.na(loinc_description), paste("miss_desc", test_name, fluid, category, sep="_"), loinc_description))

# Create 'Tuple' (event description, flag) encoded as "event-flag"
labs_labels = labs_labels %>% 
  mutate(event_flag = paste(loinc_description, flag, sep="-"))

# count how many times each lab event_flag has ocurred, creating proc_count column
labs_labels = labs_labels %>% 
  group_by(event_flag) %>%
  mutate(event_flag_qty = n())

# Get distinct event_flag, and sort by count
lab_top = labs_labels %>% 
  distinct(event_flag, .keep_all = TRUE) %>%
arrange(desc(event_flag_qty)) %>%
  select(hadm_id, itemid, event_flag, event_flag_qty)

cat("Total unique lab event-flag combinations: ", dim(lab_top)[1], "\n")

# Basic data summary - event count distribution
summary(lab_top$event_flag_qty)

# Top ten
lab_top %>% 
  select(event_flag, event_flag_qty) %>%
  head(10)

# bottom ten
lab_top %>% 
  select(event_flag, event_flag_qty) %>%
  tail(10)
```

**Again we see the outputs of the top ten lab events which occurred among the population of hospital admissions.  Evident to see that many more lab events occurr than procedures - which is intuitive. Interesting to see the blend of abnormal to normal flags inculded with the event, and to see that trends like "while there is high count of 'Hemoglobin-abnormal' instances, 'Hemoglobin-normal' is not part of the top ten". Comparing this table with the procedures table helps show the magnitude difference and need for the log transformation**

```{r}
# cleaning up workspace...
rm(l_top10, d_labitem, labevents, labev_drg)

```

### Create Data Frame for Deep Learning Model
Here we build the dataset that will be fed into the NN model later on. Instructed to keep the top 2,000 potential features from each table, we will keep all features since each table had less than 1,000 each.  It's evident that the frequency of events per hospital admin is quite sparse.  We will log transform the data, both features and target following f(x) = log(1+x).
```{r}
# from our dataframe with all events, get the those that are part of top-procedure events
proc_events =  drg_proc %>%
  filter(description %in% proc_top$description) %>%
  select(hadm_id, description)

# rename column for consistency
proc_events = rename(proc_events, event = description)

# count where hospital admission incurred a given event (hadm_id, event) 
proc_events = proc_events %>%
  group_by(hadm_id, event)%>%
  mutate(event_count = n())

# from our dataframe with all lab events, get the those that are part of top-lab events
lab_events = labs_labels %>%
  filter(itemid %in% lab_top$itemid) %>%
  select(hadm_id, event_flag)

# rename column for consistency
lab_events = rename(lab_events, event = event_flag)

# count where hospital admission incurred a given event (hadm_id, event) 
lab_events = lab_events %>%
  group_by(hadm_id, event)%>%
  mutate(event_count = n())

# combine the two dfs into a single dataframe
combined = bind_rows(proc_events,lab_events)
cat("combined dimension\n rows/instances: ", dim(combined)[1], "\n columns/variables: ", dim(combined)[2], "\n")

# Transpose/go from Long to Wide - Spread
dataset = combined %>% 
  distinct(.keep_all = TRUE) %>%
  spread(event, event_count, fill=0)
cat("dataset dimension\n examples/instances: ", dim(dataset)[1], "\n features/variables: ", dim(dataset)[2], "\n")

# add cost weight to dataframe
dataset = dataset %>%
  inner_join(select(drgevents, hadm_id, cost_weight), by="hadm_id")

cat("dataset dimension w/ cost_weight\n examples/instances: ", dim(dataset)[1], "\n features/variables: ", dim(dataset)[2], "\n")

# log transform the dataset - hadm_id is furthest left column 1 and cost_weight is furthest right 1786
dataset[,2:1786] = log(dataset[,2:1786]+1)

```
**The outputs show that we will have 1785 features in the model, with 5,050 examples/instances.  The data was log transformed as per f(x) = log(1+x)**
**The frequency of lab events**

```{r}
# cleaning up workspace...
rm(combined, drg_proc, drgevents, lab_events, lab_top, labs_labels, proc_events, proc_top)
```
##### Create the 50/50 Train Test Sets
```{r}
# ungroup dataset so we can delete hadm_id from data that will be loaded into model
dataset = dataset %>%
  ungroup() 

# get random 50/50 split, based on hadm_id - the unique examples
library(caTools)
set.seed(123)
split = sample.split(dataset$hadm_id, SplitRatio = 0.5)
train = subset(dataset, split == TRUE)
test = subset(dataset, split == FALSE)
# delete the hadm_id
train = subset(train, select=-hadm_id)
test = subset(test, select=-hadm_id)
# Test set
xtest = test %>% select(-cost_weight) %>% as.matrix()
ytest = test %>% select(cost_weight) %>% as.matrix()
# Train set
xtrain = train %>% select(-cost_weight) %>% as.matrix()
ytrain = train %>% select(cost_weight) %>% as.matrix()

cat("Train Set dimensions \n examples/instances: ", dim(xtrain)[1], "\n features/variables: ", dim(xtrain)[2], "\n")
cat("Test Set dimensions \n examples/instances: ", dim(xtest)[1], "\n features/variables: ", dim(xtest)[2], "\n")
```

### Create NN models - Instructed to make 3 hidden layers of size 32 
#### Baseline
```{r}
library(ggplot2)
library(keras)
### Specify the architecture
baseline = keras_model_sequential() 
baseline %>%
  layer_dense(units = 32, activation = 'tanh',
                input_shape = c(ncol(xtrain))) %>%   
  #layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 32, activation = 'tanh') %>%
  layer_dense(units = 1)

summary(baseline)

### Specify loss, batch size, optimizer, extra performance measures
baseline %>% compile(
  loss = c('mse'),
  optimizer = optimizer_nadam(clipnorm = 10),
  metrics = c('mse')
)

```

**For fitting the regression curve, we would want to reduce square error, so I'm using MSE as the loss function and metric for all test cases. Using nadam for optimization function - clipping while I'm at it... I tested a handful of different combinations of activation functions, and tweeked some hyper parameters with regularization.**  

**Relu seems like a good choice for combating vanishing gradients, but it alone at each layer has issues...so I'm going to use tanh in combination to comabat relu from calculating us to the zero problem **

### 1f. Train the model, report train 

**In the following section, I'll compare and contrast the baseline model to a L1 and L2 regularized models. I'm keeping batch size at 200 and epochs at 30, as this seemed like an OK ~median to work with**
**I'm keeping the activation functions as "mixed" (tanh, relu, tanh) since these seemed to create a better "fit" to estimating our outcome "cost_weight" better than keeping activation functions the same across layers (ie. relu, relu, relu)**

```{r}
### Run baselinnne model
base_history = 
  baseline %>% fit(xtrain, ytrain,
              epochs = 30,
              batch_size = 200,
              validation_split = 0.2, shuffle=T
  )

```

##### Check Out some Regularization to compare to baseline

```{r}
### L1 - Lasso 
l1_reg = keras_model_sequential() 
l1_reg %>%
  layer_dense(units = 32, activation = 'tanh',
                input_shape = c(ncol(xtrain)),
              kernel_regularizer = regularizer_l1(l = 0.01)) %>%   
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 32, activation = 'tanh', 
              kernel_regularizer = regularizer_l1(l = 0.05)) %>%
  layer_dense(units = 1)

#Same dimensions...
#summary(l1_reg)

# Mean Square error as loss and metric to evalutate
l1_reg %>% compile(
  loss = c('mse'),
  optimizer = optimizer_nadam(clipnorm = 10),
  metrics = c('mse')
)

### L2 - Ridge 
l2_reg = keras_model_sequential() 
l2_reg %>%
  layer_dense(units = 32, activation = 'tanh',
                input_shape = c(ncol(xtrain)),
              kernel_regularizer = regularizer_l2(l = 0.01)) %>%   
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 32, activation = 'tanh', 
              kernel_regularizer = regularizer_l2(l = 0.05)) %>%
  layer_dense(units = 1)

#Same dimensions...
#summary(l2_reg)

# Mean Square error as loss and metric to evalutate
l2_reg %>% compile(
  loss = c('mse'),
  optimizer = optimizer_nadam(clipnorm = 10),
  metrics = c('mse')
)
```

#### L1 Run and Plot
```{r }
### Run L1 Model
l1_history = 
  l1_reg %>% fit(xtrain, ytrain,
              epochs = 30,
              batch_size = 200,
              validation_split = 0.2, 
              shuffle=T,
              verbose = F
  )
```

#### Plot L1 to baseline Comparisons
```{r}
library(tibble)
compare_l1 <- data.frame(
  baseline_train = base_history$metrics$loss,
  baseline_val = base_history$metrics$val_loss,
  l1_train = l1_history$metrics$loss,
  l1_val = l1_history$metrics$val_loss
) %>%
  rownames_to_column() %>%
  mutate(rowname = as.integer(rowname)) %>%
  gather(key = "type", value = "value", -rowname)
  
ggplot(compare_l1, aes(x = rowname, y = value, color = type)) +
  geom_line() +
  xlab("epoch") +
  ylab("loss")

```

##### L2 Run & Plot
```{r}
### Run L2 model
l2_history = 
  l2_reg %>% fit(xtrain, ytrain,
              epochs = 30,
              batch_size = 200,
              validation_split = 0.2, 
              shuffle=T,
              verbose = F
  )
```

```{r}

compare_l2 <- data.frame(
  baseline_train = base_history$metrics$loss,
  baseline_val = base_history$metrics$val_loss,
  l2_train = l2_history$metrics$loss,
  l2_val = l2_history$metrics$val_loss
) %>%
  rownames_to_column() %>%
  mutate(rowname = as.integer(rowname)) %>%
  gather(key = "type", value = "value", -rowname)
  
plotL2 = ggplot(compare_l2, aes(x = rowname, y = value, color = type)) +
  geom_line() +
  xlab("epoch") +
  ylab("loss")
plotL2
```

#### Side-by-side comparisons: Baseline, L1, L2
```{r}
# baseline Evaluating and Plotting
baseline %>% evaluate(xtest, ytest)
plot(base_history, main="base_history")

data.frame(Y_truth = ytest[,1], Y_prediction = baseline %>% predict(xtest)) %>%
  ggplot(., aes(x=Y_prediction, y=Y_truth)) +
  ggtitle("Baseline Estim")+
  geom_point()

# L1 Evaluating and Plotting
l1_reg %>% evaluate(xtest, ytest)
plot(l1_history, main="l1_history")

data.frame(Y_truth = ytest[,1], Y_prediction = l1_reg %>% predict(xtest)) %>%
  ggplot(., aes(x=Y_prediction, y=Y_truth)) +
  ggtitle("L1 Estimations")+
  geom_point()

# L2 Evaluating and Plotting
l2_reg %>% evaluate(xtest, ytest)
plot(l2_history, main="l2_history")

data.frame(Y_truth = ytest[,1], Y_prediction = l2_reg %>% predict(xtest)) %>%
  ggplot(., aes(x=Y_prediction, y=Y_truth)) +
  ggtitle("L2 Estimations")+
  geom_point()

```

**Based on the comparisions of plotting the error and cost_weight predictions, it looks like the L1 regularization model fit a better curve and estimated the costs. L2 still outperformed baseline**

**We could probably tweek some parameters/hyperparameters and get improved results, but these regularized regression models appear to be estimating the costs fairly well.**

**It would be great business-use case for the coders/billers at the hospital**
```{r}
#clean up part 1 workspace
rm(base_history, compare_l1, compare_l2, dataset, l1_history, l2_history, plotL2, test, train, xtest, xtrain, ytest, ytrain, baseline,l1_reg, l2_reg)
```

## Part 2: Classifying Pap smear slides as normal/cancerous (10 points)
### 2a. Load the data and display

```{r }
library(keras)
library(abind)
#  Going to use smaller model 'application_vgg16' as this one just takes for e ver
# # create the base pre-trained model
# base_model <- application_inception_v3(weights = 'imagenet', include_top = FALSE)

# image data, from: http://orbit.dtu.dk/files/2528109/oersted-dtu2886.pdf
image_normal_dir = paste(directory,"/imagePull/normals/", sep='')
image_cancer_dir = paste(directory,"/imagePull/carcinoma_in_situ/", sep='')

load_images = function(image_dir, dims=c(224,224,3), verbose=F) {
  imgs = array(0, dim=c(length(list.files(image_dir)),dims))
  fi=0
  for (f in list.files(image_dir)) {
    if(verbose && fi%%10 == 0) {
      print(paste0(fi, "/", length(list.files(image_dir)))) }
    fi = fi + 1
    img <- image_load(paste0(image_dir,f), target_size = dims[1:2])
    x <- image_to_array(img)
    # ensure we have a 4d tensor with single element in the batch dimension,
    # the preprocess the input for prediction using resnet50
    x <- array_reshape(x, c(1, dim(x)))
    x <- imagenet_preprocess_input(x, mode="tf")
    imgs[fi,,,] = x
  }
  return(imgs)
}
normals = load_images(image_normal_dir, verbose=T)
cancers = load_images(image_cancer_dir, verbose=T)
alldata = abind(normals,cancers, along = 1)
labels = 
  data.frame(label=c(rep("normal", dim(normals)[1]),
                     rep("cancer", dim(cancers)[1]))) %>%
  as_tibble()


# Create train test split
set.seed(1e8)
traini = sample.int(n=dim(alldata)[1], size=200)
train_data = alldata[traini,,,]
test_data = alldata[-traini,,,]
train_labels = labels[traini,][[1]]
test_labels = labels[-traini,][[1]]
rm(alldata)

# plot a few examples
par(mfrow=c(5,4), oma = c(2, 2, 0.2, 0.2), mar=c(0,1,1,1))
for(i in 1:10) {
  ((normals[i,,,] - min(normals))/(max(normals)-min(normals))) %>% 
    as.raster() %>% plot()

  ((cancers[i,,,] - min(cancers))/(max(cancers)-min(cancers))) %>% 
    as.raster() %>% plot()
}
rm(normals,cancers,labels)
```
**Not consistently. Normal cells seem to have a smaller dense dot, I'm guessing is the nucleus where as the cancer cells seem to be more round and circular, however the discrepancies in shape, size, and density aren't distinct enough to me.  but I'm glad we have Tensorflow to help out!**

#### 2b Train a Multi-layer Network: "Primal Model"
...transform our labels...
```{r}
# convert the train and test to binary outcomes vectors
Y_train =  ifelse(train_labels=="cancer", 1, 0)
Y_test = ifelse(test_labels=="cancer", 1, 0)
# remove waste
rm(train_labels, test_labels)
```
**The model has 5 layers: 128, 64, 32, 16, 1 using tanh, relu, tanh, relu, and sigmoid activation functions respectively.  An L2 regulizer is applied in the middle layer. Further, the model uses a binary cross entropy loss function since we have binary outcomes of interest, adam optimizer with default learning rate, and is tracking accuracy as the metric of interest. The model trains on the data for 30 epochs and batch size of 50**

```{r}
### Sunday Before Church
sunday = keras_model_sequential()
sunday %>% layer_flatten(input_shape=c(224,224,3))  %>%
    layer_dense(units = 64, activation = 'relu', kernel_regularizer = regularizer_l2(l = 0.01))%>%
  layer_dense(units = 32, activation = 'tanh') %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')


#  layer_dense(units = 64, layer_activation_leaky_relu(alpha=0.1), kernel_regularizer = regularizer_l2(l = 0.01)) %>%
#  layer_dense(units = 32, activation = 'tanh', kernel_regularizer = regularizer_l2(l = 0.01)) %>%
#  layer_dense(units = 16, layer_activation_leaky_relu(alpha=0.1)) %>%
#  layer_dense(units = 1, activation = 'sigmoid')



#freeze_weights(sunday, from = 3, to = 4)
summary(sunday)
# Compile the model:
## Loss = binary_crossentropy, Adam optimizer, metrics = Accuracy
sunday %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
```


```{r}
# Run 30 epochs with batch size at 25% of training set size
sunday_hist = sunday %>% fit(train_data, Y_train,
                          epochs = 30,
                          batch_size = 50,
                          validation_split = 0.2,
                           shuffle=T,
                          verbose = F)

# plot the model performance
plot(sunday_hist, main="Image Model")

# Evaluating the model on train and test sets
print("Train Set")
sunday %>% evaluate(train_data, Y_train)
p_train = sunday %>% predict_classes(train_data)
table(Predicted = p_train, Actual=Y_train)
print("Test Set")
sunday %>% evaluate(test_data, Y_test)
su_test= sunday %>% predict_classes(test_data)
table(Predicted = su_test, Actual=Y_test)s
#rm(sunday, sunday_hist)
```

```{r}
sunday %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(lr=0.0001),
  metrics = "accuracy"
)



unfreeze_weights(sunday, from = 1, to=6)
### RECOMPILE
sunday %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
summary(sunday)
```

```{r}
#### RE Run
# Run 30 epochs with batch size at 25% of training set size
sunday_hist = sunday %>% fit(train_data, Y_train,
                          epochs = 30,
                          batch_size = 50,
                          validation_split = 0.3,
                           shuffle=T,
                          verbose = F)

# plot the model performance
plot(sunday_hist, main="Image Model")

# Evaluating the model on train and test sets
print("Train Set")
sunday %>% evaluate(train_data, Y_train)
p_train = sunday %>% predict_classes(train_data)
table(Predicted = p_train, Actual=Y_train)
print("Test Set")
sunday %>% evaluate(test_data, Y_test)
su_test= sunday %>% predict_classes(test_data)
table(Predicted = su_test, Actual=Y_test)
```



```{r}
### Submission Architecture
primal = keras_model_sequential()
primal %>% layer_flatten(input_shape=c(224,224,3))  %>%
  layer_dense(units = 128, activation = 'tanh') %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 32, activation = 'tanh', kernel_regularizer = regularizer_l2(l = 0.005)) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Compile the model:
## Loss = binary_crossentropy, Adam optimizer, metrics = Accuracy
primal %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
# Plot the Model Summary
summary(primal)
```
```{r}
# Run 30 epochs with batch size at 25% of training set size
primal_hist = primal %>% fit(train_data, Y_train,
                          epochs = 30,
                          batch_size = 50,
                          validation_split = 0.2,
                           shuffle=T,
                          verbose = F)

# plot the model performance
plot(primal_hist, main="Image Model")

# Evaluating the model on train and test sets
print("Train Set")
primal %>% evaluate(train_data, Y_train)
p_train = primal %>% predict_classes(train_data)
table(Predicted = p_train, Actual=Y_train)
print("Test Set")
primal %>% evaluate(test_data, Y_test)
pri_test= primal %>% predict_classes(test_data)
table(Predicted = pri_test, Actual=Y_test)
```
**I tested many combintations of 3-5 layers, qty of units per layer, which types of activation functions to use and at which layers, and placement of L1 and L2 regulizers.  After hours of playing with this, I realized that overall accuracy remained between 55-75% with an eyeball mean of 63%. Without regularization, the models had tendencies to predict high counts for True Normal cells (eg 100), but very low counts for True Cancer cells (eg 19). With regularization, the models more easily identified cancer, but also falsely identified cancer.**
clean working station...
```{r}
rm(primal_hist, p_train)
```

#### 2c Fine Tuning model

Through 'trial at random' (TAR) I found out that calling **freeze(model)** does not actually freeze any parameters in the model.  You have to explicitly give the 'from' 'to' args to freeze weights/parameters
TAR also exposed me to the utility of keras::sequential_model()'s .predict_classes() so that's how I'm setting up my fine-tuned model. 

**Custom Layers:**  I've added 4 custom layers of 64, 32, 16, 1 with activation functions relu, tanh, relu, and sigmoid respectively. I also have teh L2 regularizer on the middle layer.
**Compiled as:**  Keep same set up - binary cross entropy, adam optimization, accuracy  
**Trained as:** initilizing uses 5 epochs, actual training uses 10 epochs. Same batch size as previous

My approach is:  
* read in base model, application_vgg16
* transform base model, add Custom layers
* freeze the base model layers/parameters
* compile the model and initilize the Custom layers through training on data, epoch of 5
* unfreeze Convolution Block 5, which borders Custom layers
* lower the learning rate on Adam optimizer
* Train the fine-tuned model

```{r}
# get base_model with input_shape arguement - compatibility 
base_model <- application_vgg16(weights = 'imagenet', include_top = FALSE,
                                       input_shape = c(224,224,3))
# Add our custom layers
predictions <- base_model$output %>% 
  layer_flatten(input_shape=c(224,224,3))  %>%
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = 32, activation = 'tanh', kernel_regularizer = regularizer_l2(l = 0.01))%>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

# the "Fine-tuned Model" as Model
fu_model <- keras_model(inputs = base_model$input, outputs = predictions)
# the "Fine-tuned Model" as Keras Sequential Model
fine_tuned = keras::keras_model_sequential(layers = fu_model$layers)
print("Model Model")
summary(fu_model)
print("Sequential Model")
summary(fine_tuned)
rm(fu_model)

# let's visualize base_model layer names and layer indices to see how many layers
layers = base_model$layers
for (i in 1:length(layers)) { 
  cat(i, layers[[i]]$name, "\n") 
}

# freeze_weights(base_model) alone doesn't freeze .... 
freeze_weights(base_model)
cat("Trainable weights before pre-train freeze: ",length(fine_tuned$trainable_weights), "/n")
# Explicitly call from 1 to 19 - which are all the weights in base_model
freeze_weights(base_model, from = 1, to =19)
cat("Trainable weights after explicit freeze: ",length(fine_tuned$trainable_weights), "/n")

# compile the model 
fine_tuned %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
# Get the model summary - look at all them non-trainable parameters!
summary(fine_tuned)

# Conduct initial training on 'unfreezed' weights - initialize them to avoid bad backpropogation
initialize = fine_tuned %>% fit(x=train_data, y=Y_train,
                          epochs = 5,
                          batch_size = 50,
                           shuffle=T)

# plot the initialization performance
plot(initialize, main="Image Model")
```
```{r}
rm(initialize, layers)
```

##### Recompile and Train model 
```{r}
# We will freeze all the pre-trained instances (1 - 19) as a safety net
# and fine-tune the model at index 16, 'block5_conv1'
# all of block5 with the new layers will then be trained on the data
freeze_weights(base_model, from = 1, to =19)
cat("Trainable weights before pre-train unfreeze: ",length(fine_tuned$trainable_weights), "/n")
unfreeze_weights(base_model, from = 16)
cat("Trainable weights after pre-train unfreeze: ",length(fine_tuned$trainable_weights), "/n")

# recompile the model for these modifications to take effect
# optimizer_adam()'s base learning rate is 0.001, so i'm guessing 'adam' default is 0.001
# since we already had a good initialization results, I'm reducing Adam's learning 
fine_tuned %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(lr=0.0001),
  metrics = "accuracy"
)
summary(fine_tuned)
# train the model on the new data for a few epochs
fine_hist = fine_tuned %>% fit(train_data, Y_train,
                          epochs = 25,
                          batch_size = 50,
                          validation_split = 0.2,
                           shuffle=T,
                          verbose = F)

```

##### Plot Perfomance
```{r}
# plot the fine_tune 'learning'
plot(fine_hist, main="Image Model")

# Evaluating the model on train and test sets
print("Train Set")
fine_tuned %>% evaluate(train_data, Y_train)
p_train = fine_tuned %>% predict_classes(train_data)
table(Predicted = p_train, Actual=Y_train)
print("Test Set")
fine_tuned %>% evaluate(test_data, Y_test)
f_test = fine_tuned %>% predict_classes(test_data)
table(Predicted = f_test, Actual=Y_test)
```
#### 2d Compare ROC and AUC for Fine-tuned and 'Primal' models
```{r}
library(ROCR)
# Primal Model's ROC and AUC
pmodel = ROCR::prediction(pri_test, Y_test)
p_per = ROCR::performance(pmodel, "tpr", "fpr" )
p_auc = ROCR::performance(pmodel,"auc")
p_avg_auc = mean(unlist(p_auc@y.values))

# Fine-Tuned Model's ROC and AUC
ftmodel = ROCR::prediction(f_test, Y_test)
ft_per = ROCR::performance(ftmodel, "tpr", "fpr" )
ft_auc = ROCR::performance(ftmodel,"auc")
ft_avg_auc = mean(unlist(ft_auc@y.values))

cat("Primal model's AUC: ", p_avg_auc, "/n")
cat("Fine-tuned model's AUC: ", ft_avg_auc, "/n")
```
```{r}
#Plotting Primal's ROC Curve
plot(p_per, main="Primal", avg="vertical",pin=c(10,10))
abline(a=0, b=1, lty=2)

```
```{r}
cat("Fine-tuned model's AUC: ", ft_avg_auc, "/n")
#Plotting Fine Tuned ROC Curve
plot(ft_per,  main="Fine-Tuned", avg="vertical",pin=c(10,10))
abline(a=0, b=1, lty=2)
```

**Clearly, fine-tuning an existing model can add a lot of value for this use case of detecting cancer. Across the activities of this section, it outperformed the primal model in every metric. It's cool to see that the fine-tune uses 1/3 of epochs and still comes out impressively better than Primal. I'm interested to see if a stronger initilization or hyperparameter tuning could get this machine humming**

#### 2e Analysis to Share with an Expert

**Though there could be more done to it, the fine-tuned model did really well and could help clinicians in detecting cancer.**
**ML Helping the Clinician**  
I would show the clinician Model's summary that shows qty of layers and parameters the neural net is using to, as this answers 'What' is being employed.  For 'How'/'How much, how effective' factor, I would show the clinician the confusion matrix and the ROC curves as these two are the more intuitive reports that help paint the picture of contrasting the Model's ability to accurately/inaccurately classify cancer and assess bias.
**My Request from Clinician**  
To help the model improve, I would want more images to train the model and also understand the magnitude of the cost of error in classifying FPs and FNs to side with model error (bias, etc) that has the better outcome.

```{r}
####### Convolutional Networl

#primal %>% layer_flatten(input_shape=c(224,224,3))  %>%
# Initialize sequential model
cnn_test <- keras_model_sequential()

cnn_test %>%
 
  # Start with hidden 2D convolutional layer being fed 32x32 pixel images
  layer_conv_2d(
    filter = 64, kernel_size = c(3,3), padding = "same", 
    input_shape=c(224,224,3), activation = 'relu') %>%
  layer_conv_2d(filter = 64, kernel_size = c(3,3), activation = 'relu') %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # 2 additional hidden 2D convolutional layers
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same", activation = 'relu') %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), activation = 'relu') %>%

  # Use max pooling once more
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten(input_shape=c(224,224,3)) %>%

  # Outputs from dense layer are projected onto 10 unit output layer
  layer_dense(units = 32, activation = 'tanh', kernel_regularizer = regularizer_l2(l = 0.01))%>%
 layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')


summary(cnn_test)

```

```{r}
###### Compile and Test

# compile the model 
cnn_test %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
# Conduct initial training on 'unfreezed' weights - initialize them to avoid bad backpropogation
initialize = cnn_test %>% fit(x=train_data, y=Y_train,
                          epochs = 20,
                          batch_size = 50,
                          validation_split = 0.2,
                           shuffle=T)
```



```{r}
####### WEBSITE
model <- keras_model_sequential()

model %>%
 
  # Start with hidden 2D convolutional layer being fed 32x32 pixel images
  layer_conv_2d(
    filter = 32, kernel_size = c(3,3), padding = "same", 
    input_shape = c(32, 32, 3)
  ) %>%
  layer_activation("relu") %>%

  # Second hidden layer
  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%
  layer_activation("relu") %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # 2 additional hidden 2D convolutional layers
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same") %>%
  layer_activation("relu") %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%
  layer_activation("relu") %>%

  # Use max pooling once more
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
  
  # Flatten max filtered output into feature vector 
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(512) %>%
  layer_activation("relu") %>%
  layer_dropout(0.5) %>%

  # Outputs from dense layer are projected onto 10 unit output layer
  layer_dense(10) %>%
  layer_activation("softmax")
summary(model)
```

